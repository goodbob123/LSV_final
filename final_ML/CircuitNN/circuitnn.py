"""CircuitNN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19ffsYpcjQexHujEwW7sjpQ9JYu2xenbN
"""

import torch
import torch.nn as nn
import numpy as np
import math
from torch.nn import functional as F
from torch.utils.data import DataLoader, Dataset
from tqdm.auto import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"

"""# Gumble softmax
[link1](https://zhuanlan.zhihu.com/p/144140006)
"""

# raw_data = ["0110100101101001100101101001011010010110100101100110100101101001",
#        "0010101100101011000000100000001000000010000000100000000000000000",
#        "0010101100101011010000100100001001000010010000101101010011010100",
#        "1101010011010100111111011111110111111101111111011111111111111111",
#        "0000000011111111000000001111111100000000111111110000000011111111"]
# raw_data = ["0101010110011001"]
raw_data = ["1001"]

with open("TT.txt",'w') as f:
  for output in raw_data:
    f.write(output)
    f.write('\n')

class TT(Dataset):
    def __init__(self, tt_path):
      self.TT = []
      with open(tt_path,'r') as f:
        for line in f.readlines():
          for i, ch in enumerate(line[:-1]):
            if len(self.TT) == i: self.TT.append([])
            if (ch == '0'): self.TT[i].append(0.0)
            elif (ch == '1'): self.TT[i].append(1.0)
            else: continue
      #print(self.TT)
      self.y = torch.tensor(self.TT, dtype = torch.float)
      #print(self.y)

      vec_len = int(math.log(len(self.TT),2))
      self.x = torch.zeros((len(self.TT),vec_len), dtype = torch.float)
      for mt in range(len(self.TT)):
        mt_ = mt
        j = 0
        while (mt_ != 0):
          if (mt_%2): self.x[mt][vec_len - j - 1] = 1.0
          # else: self.x[mt][j] = 0.0
          j = j+1
          mt_ = mt_//2
      #print(self.x)


    def __getitem__(self, index):
        return self.x[index], self.y[index]
    def __len__(self):
        return self.x.shape[0]

tt = TT("TT.txt") # no need for val and test set
# train_batch_size = 32
train_batch_size = 4
train_tt = DataLoader(tt, batch_size=train_batch_size, shuffle=True)
val_tt = DataLoader(tt, batch_size=1, shuffle=False)
# val_tt = DataLoader(tt, batch_size=1, shuffle=False)

# for in_vec, out_vec in tqdm(val_tt, disable = True):
#   in_vec = in_vec.to(device)
#   out_vec = out_vec.to(device)
#   print(in_vec)
#   print(out_vec)

"""[prob matrix](https://medium.com/@pumplerod/probabilistic-neural-network-with-pytorch-11ec04479f67)"""

class fc_gate(nn.Module):
  is_eval = False
  is_bias_w = False
  is_onehot = False
  def __init__(self, nPI, nFI, nFO, cool_rate = 1e-8, first_layer = False, last_layer = False):
    super(fc_gate, self).__init__()
    if (first_layer and last_layer):
        print("error statement")
        raise
    # self.is_onehot = is_onehot
    self.nPI = nPI
    self.nFI = nFI
    self.nFO = nFO
    self.num_gtype = 3

    if (first_layer): layer_nFI = 2 * nPI
    else: layer_nFI = 2 * (self.num_gtype * nFI + nPI)
    if (last_layer): layer_nFO = nFO
    else: layer_nFO = self.num_gtype * nFO

    self.fc_1 = nn.Linear(int(layer_nFI), int(layer_nFO), bias = fc_gate.is_bias_w)
    self.fc_2 = nn.Linear(int(layer_nFI), int(layer_nFO), bias = fc_gate.is_bias_w)
    self.I = torch.eye(layer_nFI, requires_grad = False).to(device)

    self.tau = 1
    self.cool_rate = cool_rate
    self.is_onehot = fc_gate.is_onehot
    self.first_layer = first_layer
    self.last_layer = last_layer
  def set_connect(self, connection):
    fc_1_init = torch.zeros_like(self.fc_1.weight)
    fc_2_init = torch.zeros_like(self.fc_2.weight)
    for c in connection:
      fo = c[0]
      fi_1 = c[1]
      fi_2 = c[2]
      fc_1_init[fo][fi_1] = 1
      fc_2_init[fo][fi_2] = 1
    self.fc_1.weight = torch.nn.Parameter(fc_1_init)
    self.fc_2.weight = torch.nn.Parameter(fc_2_init)
    return
  def forward(self, FI):
    comp_FI = torch.ones(FI.size(dim=1)).to(device) - FI
    FI = torch.cat((FI,comp_FI),1)

    w_1 = self.fc_1(self.I)
    w_1 = w_1.T
    w_1 = F.softmax(w_1,dim = 1) # is this correct?

    w_2 = self.fc_2(self.I)
    w_2 = w_2.T
    w_2 = F.softmax(w_2, dim = 1)

    if (fc_gate.is_eval):
      FI_id_1 = w_1.argmax(1)
      prob_FI_1 = torch.zeros_like(w_1).scatter_(1, FI_id_1.unsqueeze(1), 1.).to(device)
      FI_id_2 = w_2.argmax(1)
      prob_FI_2 = torch.zeros_like(w_2).scatter_(1, FI_id_2.unsqueeze(1), 1.).to(device)
    else:
      logit_1 = torch.log(w_1)
      prob_FI_1 = F.gumbel_softmax(logit_1, tau=self.tau, hard = self.is_onehot)
      logit_2 = torch.log(w_2)
      prob_FI_2 = F.gumbel_softmax(logit_2, tau=self.tau, hard = self.is_onehot)
    # if (is_eval and (self.first_layer or self.last_layer)):
    #   print("w_1:",torch.max(w_1))

    prob_ones_FI_1 = torch.matmul(FI,prob_FI_1.T)
    prob_ones_FI_2 = torch.matmul(FI,prob_FI_2.T)
    if (self.last_layer): return prob_ones_FI_1

    # if (is_eval and (self.first_layer or self.last_layer)):
      # print("w_2:",torch.max(w_2))

    if (not fc_gate.is_eval):
      self.tau = self.tau * (1 - self.cool_rate);

    FO_AND = prob_ones_FI_1.T[:self.nFO] * prob_ones_FI_2.T[:self.nFO]
    FO_OR = 1 - ((1 - prob_ones_FI_1.T[self.nFO : 2 * self.nFO]) * (1 - prob_ones_FI_2.T[self.nFO : 2 * self.nFO]))
    FO_XOR = (prob_ones_FI_1.T[2 * self.nFO:] * (1 - prob_ones_FI_2.T[2 * self.nFO :])) + ((1 - prob_ones_FI_1.T[2 * self.nFO:]) * prob_ones_FI_2.T[2 * self.nFO :])
    FO = torch.cat((FO_AND.T, FO_OR.T, FO_XOR.T),1)
    return FO

class Aig(nn.Module):
  def __init__(self):
    super().__init__()
    self.is_eval = False #is_eval
    # self.is_onehot = False
    self.lv_1 = fc_gate(2, 0, 2, first_layer = True)
    # print(self.lv_AND1.fc_1.weight.data.size())
    self.lv_2 = fc_gate(2, 2, 1)
    self.lv_PO = fc_gate(2, 3, 1, last_layer = True)
  # def set_connect(self):
    # self.lv_1.set_connect([(0,1,0),(4,0,3)])
    # self.lv_2.set_connect([(0,11,8)])
    # self.lv_3.set_connect([(1,4,10)])
    # self.lv_PO.set_connect([(0,14,1)])
  def forward(self,FI):
    fc_gate.is_eval = self.is_eval
    FI = torch.flip(FI,(0,1))

    FO = self.lv_1(FI)
    FI = torch.cat((FI, FO),1)

    FO = self.lv_2(FI)
    FI = torch.cat((FI, FO),1)

    PO = self.lv_PO(FI)
    FI = PO
    return FI

model = Aig().to(device)
# model.set_connect()
model.train()
opt = torch.optim.SGD(model.parameters(), lr=5e-2, momentum = 0.9)
lambda_schedule = lambda epo : (1 - 1e-3)**epo if epo < 1e+3 else (1e-1)*(1 - 1e-6)**epo if epo < 1e-7 else 1e-6
scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda = lambda_schedule)
loss = nn.MSELoss()

num_epo = 10000000
high_acc = 0
epo = 0
while True:
  epo += 1
  for in_vec, out_vec in tqdm(train_tt, disable = True):
    in_vec = in_vec.to(device)
    out_vec = out_vec.to(device)
    ckt_out = model(in_vec)
    l = loss(ckt_out, out_vec)
    l.backward()

    opt.step()
    scheduler.step()
    opt.zero_grad()
  if (epo%10000==0):
    print(l)
    model.eval()
    model.is_eval = True
    # print(m.modules)
    with torch.no_grad():
      l = 0
      correct_cnt = torch.zeros((1,1), dtype = torch.float).to(device)
      for in_vec, out_vec in tqdm(val_tt, disable = True):
        in_vec = in_vec.to(device)
        out_vec = out_vec.to(device)
        ckt_out = model(in_vec)
        l += loss(ckt_out, out_vec)
        correct_cnt += (out_vec == ckt_out)
      acc = correct_cnt[0][0] / len(val_tt)
      if (acc >= high_acc):
        high_acc = acc
        torch.save(model.state_dict(), './cktnn')
        if (acc == 1.0): break
      print(f"Validation | Epoch {epo + 1} | acc = {correct_cnt / len(val_tt)} | loss = {l}")
      # print("max_prob", torch.max(F.softmax(model.lv_1.fc_1.weight),1))
    model.train()
    model.is_eval = False

if (acc == 1.0):
  print(F.softmax(model.lv_1.fc_1.weight))
  print(F.softmax(model.lv_1.fc_2.weight))
  print(F.softmax(model.lv_2.fc_1.weight))
  print(F.softmax(model.lv_2.fc_2.weight))
  print(F.softmax(model.lv_PO.fc_1.weight))
